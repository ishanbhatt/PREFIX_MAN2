If you are running this against minikube
1) kubectl apply -f PM2_Deployment.yaml -> It will create 2 deployments canary and prod
2) Make sure you add type: NodePort in the PM2_Service.yaml
3) kubectl apply -f PM2_Service.yaml
4) kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
backend      NodePort    10.111.251.199   <none>        6757:31181/TCP   3m8s
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP          2d21h
5) Get the node-ip using minikube ip
6) curl -X GET 'http://<MINIKUBE_IP>:31181/prefix?class=e'

================================SET UP VOLUME==========================================
1) Go to the minikube node using minikube ssh
2) Create a directory sudo mkdir /mnt/data
3) Create a persistent volume.
    Here you are creating the actual volume and specifying it's size, name, hostPath: "/mnt/data"
    See in PV-Volume.yaml -> storageClassName: pv-demo (It is most useful)
    kubectl apply -f PV-Volume.yaml
    kubectl get pv
    NAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
    my-persistent-volume   100Mi      RWO            Retain           Available           pv-demo                 52s

4) Create Persistent volume claim
    Specify the storaceClassName same as the one you had created above.
    Here, you are requesting to claim 100Mi from the storageClass
    If it were available it will be granted to you.
    kubectl apply -f PV-Volume-Claim.yaml
    kubectl get pvc
    NAME                        STATUS   VOLUME                 CAPACITY   ACCESS MODES   STORAGECLASS   AGE
    my-persistent-volumeclaim   Bound    my-persistent-volume   100Mi      RWO            pv-demo        110s

    At this stage if you check PV , you will see that it's STATUS will be Bound.
    kubectl get pv
    NAME                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                               STORAGECLASS   REASON   AGE
    my-persistent-volume   1Gi        RWO            Retain           Bound    default/my-persistent-volumeclaim   pv-demo                 82m

5) Update your deployment with the volume-claim details
    Add volumes to spec (sibling of container)
        - Specify volume's name and claim's name.
    Refer to above volume's name with volumeMounts in container section
    Specify the path where you want to mount this thing and make sure your code is updated to use this DB location

==============================SET UP LOAD GENERATOR====================================
1) Inject the client with the Istio sidecar proxy so network interactions are governed by Istio:
    kubectl apply -f <(istioctl kube-inject -f samples/httpbin/sample-client/fortio-deploy.yaml)
2) Login to that pod and generate load
    FORTIO_POD=$(kubectl get pod | grep fortio | awk '{ print $1 }')
        kubectl exec -it $FORTIO_POD  -c fortio /usr/bin/fortio -- load -curl  http://backend:6757/prefix?class=e"
        kubectl exec -it $FORTIO_POD  -c fortio /usr/bin/fortio -- load -c 2 -qps 0 -n 20 -loglevel Warning http://backend:6757/prefix?class=e

    YOU WILL BE GETTING FOLLOWING OP IF YOU LOAD IT
    Sockets used: 13 (for perfect keepalive, would be 3)
    Code 200 : 19 (63.3 %)
    Code 503 : 11 (36.7 %)  # The circuit has tripped and it returned 503
    A 503 Service Unavailable Error is an HTTP response status code indicating that a server is temporarily unable to handle the request. This may be due to the server being overloaded or down for maintenance

    If you do not load it, 1 concurrent connection and 10 request
    Sockets used: 1 (for perfect keepalive, would be 1)
    Code 200 : 10 (100.0 %)

    Server Logs
    127.0.0.1 - - [02/Jan/2020 10:45:11] "GET /prefix?class=e HTTP/1.1" 200 -
    127.0.0.1 - - [02/Jan/2020 10:45:11] "GET /prefix?class=e HTTP/1.1" 200 -

    This means that server always gave 200 code, the 503 is coming by istio-envoy itself
